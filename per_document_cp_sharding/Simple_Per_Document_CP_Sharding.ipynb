{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Document-Level CP Sharding Simulation"
      ],
      "metadata": {
        "id": "HqvLxe_YefKp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAV4TRJaeaYK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Function\n",
        "\n",
        "class PerDocumentCPSharding(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, shard_id, world_size=2):\n",
        "        \"\"\"\n",
        "        CP-sharding simulation\n",
        "        - input (Tensor): [B, L, D] input tensor\n",
        "        - shard_id (int): 0 or 1\n",
        "        - world_size (int): Must be 2 (2 workers for this implementation)\n",
        "        \"\"\"\n",
        "        # Force world size to be 2\n",
        "        if world_size != 2:\n",
        "            raise ValueError(\"This implementation only supports world_size==2.\")\n",
        "\n",
        "        # Assume doc length is divisible by 4 for simplicity\n",
        "        B, L, D = input.shape\n",
        "        if L % 4 != 0:\n",
        "            raise ValueError(\"Sequence length L must be divisible by 4.\")\n",
        "        quarter = L // 4\n",
        "\n",
        "        # Save context for backward pass\n",
        "        ctx.input_shape = input.shape\n",
        "        ctx.quarter = quarter\n",
        "        ctx.shard_id = shard_id\n",
        "\n",
        "        if shard_id == 0:\n",
        "            # Worker 0: first and last quarter\n",
        "            first_part = input[:, :quarter, :]\n",
        "            last_part  = input[:, 3*quarter:, :]\n",
        "            out = torch.cat([first_part, last_part], dim=1)\n",
        "        elif shard_id == 1:\n",
        "            # Worker 1: Middle half\n",
        "            out = input[:, quarter:3*quarter, :]\n",
        "        else:\n",
        "            raise ValueError(\"shard_id must be 0 or 1.\")\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Obtain gradients for original input\n",
        "        For worker 0:\n",
        "          - The first quarter of grad_output goes to tokens 0 : quarter\n",
        "          - The second quarter goes to tokens 3*quarter : L\n",
        "        For worker 1:\n",
        "          - grad_output maps to tokens quarter : 3*quarter\n",
        "        \"\"\"\n",
        "        B, L, D = ctx.input_shape\n",
        "        quarter = ctx.quarter\n",
        "        shard_id = ctx.shard_id\n",
        "\n",
        "        # Initialize gradient tensor with zeros\n",
        "        grad_input = torch.zeros((B, L, D), device=grad_output.device, dtype=grad_output.dtype)\n",
        "\n",
        "        if shard_id == 0:\n",
        "            # grad_output shape: [B, 2*quarter, D]\n",
        "            grad_input[:, :quarter, :]      = grad_output[:, :quarter, :]\n",
        "            grad_input[:, 3*quarter:, :] = grad_output[:, quarter:, :]\n",
        "        elif shard_id == 1:\n",
        "            grad_input[:, quarter:3*quarter, :] = grad_output\n",
        "        else:\n",
        "            raise ValueError(\"Invalid shard_id in backward pass.\")\n",
        "\n",
        "        return grad_input, None, None\n",
        "\n",
        "# Wrapper\n",
        "def per_document_cp_shard(input, shard_id, world_size):\n",
        "    return PerDocumentCPSharding.apply(input, shard_id, world_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assume a batch of 2 documents, each with 16 tokens and feature dimension 8\n",
        "    B, L, D = 2, 16, 8\n",
        "    world_size = 2  # Only two shards are supported here\n",
        "\n",
        "    # Create an input tensor\n",
        "    x = torch.randn(B, L, D, requires_grad=True)\n",
        "\n",
        "    # Simulate worker 0 (shard_id=0)\n",
        "    out0 = per_document_cp_shard(x, shard_id=0, world_size=world_size)\n",
        "    loss0 = out0.sum()\n",
        "    loss0.backward(retain_graph=True)\n",
        "    print(\"Worker 0 input gradient:\")\n",
        "    print(x.grad)  # Should be nonzero only in positions 0:quarter and 3*quarter : L\n",
        "\n",
        "    # Reset gradients\n",
        "    x.grad.zero_()\n",
        "\n",
        "    # Simulate worker 1 (shard_id=1)\n",
        "    out1 = per_document_cp_shard(x, shard_id=1, world_size=world_size)\n",
        "    loss1 = out1.sum()\n",
        "    loss1.backward()\n",
        "    print(\"Worker 1 input gradient:\")\n",
        "    print(x.grad)  # Should be nonzero only in positions quarter : 3*quarter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM4Q2yHLhCFE",
        "outputId": "3b1ac8d8-5109-498c-8e15-9cc6a5046593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker 0 input gradient:\n",
            "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
            "Worker 1 input gradient:\n",
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now including varying document lengths"
      ],
      "metadata": {
        "id": "ktFFgL2Uibo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Function\n",
        "import math\n",
        "\n",
        "class PerDocumentCPShardingVarLen(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, doc_lens, shard_id, world_size):\n",
        "        \"\"\"\n",
        "        Forward pass for per-document CP sharding with variable document lengths.\n",
        "\n",
        "        For each document:\n",
        "          - Compute left_end = floor(doc_len / 4)\n",
        "          - Compute right_start = floor(3 * doc_len / 4)\n",
        "\n",
        "        Then, for each document:\n",
        "          - If shard_id == 0: output = concatenate( tokens[0:left_end], tokens[right_start:doc_len] )\n",
        "          - If shard_id == 1: output = tokens[left_end:right_start]\n",
        "\n",
        "        Since document lengths vary, outputs are padded to a common length.\n",
        "\n",
        "        Args:\n",
        "            input (Tensor): Padded input tensor of shape [B, L, D]\n",
        "            doc_lens (Tensor): 1D tensor of shape [B] with true lengths of each document.\n",
        "            shard_id (int): Worker id (0 or 1). (Only world_size==2 is supported.)\n",
        "            world_size (int): Number of shards; must be 2.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Sharded output tensor of shape [B, max_out_len, D]\n",
        "        \"\"\"\n",
        "        if world_size != 2:\n",
        "            raise ValueError(\"This implementation only supports world_size==2.\")\n",
        "\n",
        "        B, L, D = input.shape\n",
        "        output_list = []\n",
        "        idx_info = []  # to store per-document indices for backward\n",
        "        out_lengths = []  # atctual number of tokens output for each document\n",
        "        max_out_len = 0\n",
        "\n",
        "        for i in range(B):\n",
        "            L_i = int(doc_lens[i])\n",
        "            # Compute split indices for document i\n",
        "            left_end = L_i // 4            # First quarter boundary\n",
        "            right_start = (3 * L_i) // 4     # Start of last quarter\n",
        "\n",
        "            if shard_id == 0:\n",
        "                # Worker 0 gets the first quarter and the last quarter\n",
        "                part1 = input[i, :left_end, :]\n",
        "                part2 = input[i, right_start:L_i, :]\n",
        "                out_i = torch.cat([part1, part2], dim=0)\n",
        "            elif shard_id == 1:\n",
        "                # Worker 1 gets the middle half\n",
        "                out_i = input[i, left_end:right_start, :]\n",
        "            else:\n",
        "                raise ValueError(\"shard_id must be 0 or 1.\")\n",
        "\n",
        "            cur_len = out_i.size(0)\n",
        "            max_out_len = max(max_out_len, cur_len)\n",
        "            out_lengths.append(cur_len)\n",
        "            # Store indices to use in backward\n",
        "            idx_info.append({'left_end': left_end, 'right_start': right_start, 'doc_len': L_i})\n",
        "            output_list.append(out_i)\n",
        "\n",
        "        # Pad each document’s output to have the same length (max_out_len)\n",
        "        out_tensor = input.new_zeros((B, max_out_len, D))\n",
        "        for i, out_i in enumerate(output_list):\n",
        "            cur_len = out_i.size(0)\n",
        "            out_tensor[i, :cur_len, :] = out_i\n",
        "\n",
        "        # Save information needed for the backward pass\n",
        "        ctx.idx_info = idx_info\n",
        "        ctx.out_lengths = out_lengths\n",
        "        ctx.doc_lens = doc_lens\n",
        "        ctx.shard_id = shard_id\n",
        "        ctx.input_shape = input.shape  # [B, L, D]\n",
        "\n",
        "        return out_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Backward pass: get gradients for the original padded input.\n",
        "\n",
        "        For each document i:\n",
        "          - For shard 0:\n",
        "              • Gradients corresponding to the first part (length = left_end)\n",
        "                are placed into positions [0 : left_end]\n",
        "              • Gradients corresponding to the second part (length = doc_len - right_start)\n",
        "                are placed into positions [right_start : doc_len]\n",
        "          - For shard 1:\n",
        "              • Gradients are placed into positions [left_end : right_start]\n",
        "\n",
        "        Args:\n",
        "            grad_output (Tensor): Gradient of the sharded output, shape [B, max_out_len, D]\n",
        "\n",
        "        Returns:\n",
        "            Tuple: (grad_input, None, None, None)\n",
        "                   grad_input has shape [B, L, D] corresponding to the original input\n",
        "        \"\"\"\n",
        "        B, L, D = ctx.input_shape\n",
        "        grad_input = torch.zeros((B, L, D), device=grad_output.device, dtype=grad_output.dtype)\n",
        "        idx_info = ctx.idx_info\n",
        "        shard_id = ctx.shard_id\n",
        "\n",
        "        for i, info in enumerate(idx_info):\n",
        "            doc_len = info['doc_len']\n",
        "            left_end = info['left_end']\n",
        "            right_start = info['right_start']\n",
        "\n",
        "            if shard_id == 0:\n",
        "                # Number of tokens in worker 0's output:\n",
        "                num_first = left_end               # tokens from the beginning\n",
        "                num_last = doc_len - right_start     # tokens from the end\n",
        "                # Scatter gradients for the first part\n",
        "                if num_first > 0:\n",
        "                    grad_input[i, :left_end, :] = grad_output[i, :num_first, :]\n",
        "                # Scatter gradients for the last part\n",
        "                if num_last > 0:\n",
        "                    grad_input[i, right_start:doc_len, :] = grad_output[i, num_first:num_first + num_last, :]\n",
        "            elif shard_id == 1:\n",
        "                # Worker 1 output corresponds to tokens [left_end:right_start]\n",
        "                grad_input[i, left_end:right_start, :] = grad_output[i, :right_start - left_end, :]\n",
        "\n",
        "        return grad_input, None, None, None\n",
        "\n",
        "# Wrapper\n",
        "def per_document_cp_shard_varlen(input, doc_lens, shard_id, world_size):\n",
        "    return PerDocumentCPShardingVarLen.apply(input, doc_lens, shard_id, world_size)"
      ],
      "metadata": {
        "id": "IuVtKWhSik1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example with a batch of 3 documents\n",
        "    # The padded input tensor is of shape [B, L, D], L is the maximum document length\n",
        "    B = 3\n",
        "    max_length = 20  # Maximum padded length\n",
        "    D = 8\n",
        "    # Document lengths (each document should have at least 4 tokens for the split to make sense)\n",
        "    doc_lens = torch.tensor([16, 12, 18])\n",
        "    # Create a padded input tensor (for simplicity, use random data)\n",
        "    input_tensor = torch.randn(B, max_length, D, requires_grad=True)\n",
        "\n",
        "    # Simulate processing on worker 0 (shard_id=0)\n",
        "    out_worker0 = per_document_cp_shard_varlen(input_tensor, doc_lens, shard_id=0, world_size=2)\n",
        "    # And worker 1 (shard_id=1)\n",
        "    out_worker1 = per_document_cp_shard_varlen(input_tensor, doc_lens, shard_id=1, world_size=2)\n",
        "\n",
        "    # For demonstration, compute a dummy loss on each worker's output\n",
        "    loss0 = out_worker0.sum()\n",
        "    loss1 = out_worker1.sum()\n",
        "\n",
        "    # Backpropagate (retain graph between calls for demonstration)\n",
        "    loss0.backward(retain_graph=True)\n",
        "    loss1.backward()\n",
        "\n",
        "    print(\"Sharded outputs (worker 0) shape:\", out_worker0.shape)\n",
        "    print(\"Sharded outputs (worker 1) shape:\", out_worker1.shape)\n",
        "    print(\"Expected ouput: [3, 9, 8] (3 documents, 9 tokens per document after padding, and 8 feature dimensions).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVJ6MGNKjCkW",
        "outputId": "5080cc94-a6e3-46dd-d692-fe6047cd999a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sharded outputs (worker 0) shape: torch.Size([3, 9, 8])\n",
            "Sharded outputs (worker 1) shape: torch.Size([3, 9, 8])\n",
            "Expected ouput: [3, 9, 8] (3 documents, 9 tokens per document after padding, and 8 feature dimensions).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now integrate per-document cp sharding with an attention layer"
      ],
      "metadata": {
        "id": "aJCHuN3mTnLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Function\n",
        "import math\n",
        "\n",
        "# Dummies for simulating distributed environment\n",
        "def get_distributed_rank(cp_group):\n",
        "    # For testing, assume cp_group is a dict with key 'rank'\n",
        "    return cp_group['rank']\n",
        "\n",
        "class DummyCPStream:\n",
        "    def wait_stream(self, stream):\n",
        "        pass\n",
        "\n",
        "# Per-document CP sharding (variable-length) implementation.\n",
        "class PerDocumentCPShardingVarLen(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, doc_lens, shard_id, world_size):\n",
        "        \"\"\"\n",
        "        For each document in the batch:\n",
        "          - Compute left_end = floor(doc_len / 4)\n",
        "          - Compute right_start = floor(3 * doc_len / 4)\n",
        "        Then:\n",
        "          - If shard_id == 0: output = concat( tokens[0:left_end], tokens[right_start:doc_len] )\n",
        "          - If shard_id == 1: output = tokens[left_end:right_start]\n",
        "        Since documents have variable true lengths, each document’s sharded output is padded\n",
        "        to the maximum output length in the batch.\n",
        "        \"\"\"\n",
        "        if world_size != 2:\n",
        "            raise ValueError(\"This implementation only supports world_size==2.\")\n",
        "\n",
        "        B, padded_length, D = input.shape  # padded_length is the padded sequence length\n",
        "        output_list = []\n",
        "        idx_info = []   # Stores per-document split indices (for backward)\n",
        "        out_lengths = []  # Actual output length per document.\n",
        "        max_out_len = 0\n",
        "\n",
        "        for i in range(B):\n",
        "            L_i = int(doc_lens[i])\n",
        "            left_end = L_i // 4\n",
        "            right_start = (3 * L_i) // 4\n",
        "            if shard_id == 0:\n",
        "                # Worker 0: first quarter and last quarter\n",
        "                part1 = input[i, :left_end, :]\n",
        "                part2 = input[i, right_start:L_i, :]\n",
        "                out_i = torch.cat([part1, part2], dim=0)\n",
        "            elif shard_id == 1:\n",
        "                # Worker 1: middle half\n",
        "                out_i = input[i, left_end:right_start, :]\n",
        "            else:\n",
        "                raise ValueError(\"shard_id must be 0 or 1.\")\n",
        "\n",
        "            cur_len = out_i.size(0)\n",
        "            max_out_len = max(max_out_len, cur_len)\n",
        "            out_lengths.append(cur_len)\n",
        "            idx_info.append({'left_end': left_end, 'right_start': right_start, 'doc_len': L_i})\n",
        "            output_list.append(out_i)\n",
        "\n",
        "        # Pad each document’s sharded output to have shape [max_out_len, D]\n",
        "        out_tensor = input.new_zeros((B, max_out_len, D))\n",
        "        for i, out_i in enumerate(output_list):\n",
        "            cur_len = out_i.size(0)\n",
        "            out_tensor[i, :cur_len, :] = out_i\n",
        "\n",
        "        # Save context for backward\n",
        "        ctx.idx_info = idx_info\n",
        "        ctx.out_lengths = out_lengths\n",
        "        ctx.doc_lens = doc_lens\n",
        "        ctx.shard_id = shard_id\n",
        "        ctx.input_shape = input.shape  # [B, padded_length, D]\n",
        "\n",
        "        return out_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Gets gradients from the sharded space back to the original input positions\n",
        "        The output gradient is padded to the original padded length\n",
        "        \"\"\"\n",
        "        B, padded_length, D = ctx.input_shape\n",
        "        grad_input = torch.zeros((B, padded_length, D), device=grad_output.device, dtype=grad_output.dtype)\n",
        "        idx_info = ctx.idx_info\n",
        "        shard_id = ctx.shard_id\n",
        "\n",
        "        for i, info in enumerate(idx_info):\n",
        "            doc_len = info['doc_len']\n",
        "            left_end = info['left_end']\n",
        "            right_start = info['right_start']\n",
        "            if shard_id == 0:\n",
        "                num_first = left_end\n",
        "                num_last = doc_len - right_start\n",
        "                if num_first > 0:\n",
        "                    grad_input[i, :left_end, :] = grad_output[i, :num_first, :]\n",
        "                if num_last > 0:\n",
        "                    grad_input[i, right_start:doc_len, :] = grad_output[i, num_first:num_first+num_last, :]\n",
        "            elif shard_id == 1:\n",
        "                grad_input[i, left_end:right_start, :] = grad_output[i, :right_start-left_end, :]\n",
        "        return grad_input, None, None, None\n",
        "\n",
        "def per_document_cp_shard_varlen(input, doc_lens, shard_id, world_size):\n",
        "    return PerDocumentCPShardingVarLen.apply(input, doc_lens, shard_id, world_size)\n",
        "\n",
        "# Backward helper for mapping sharded gradients back to full input\n",
        "def per_document_cp_shard_varlen_backward(grad_sharded, doc_lens, padded_length, shard_id, world_size):\n",
        "    B, _, D = grad_sharded.size()\n",
        "    grad_input = grad_sharded.new_zeros(B, padded_length, D)\n",
        "    for i in range(B):\n",
        "        doc_len = int(doc_lens[i].item())\n",
        "        left_end = doc_len // 4\n",
        "        right_start = (3 * doc_len) // 4\n",
        "        if shard_id == 0:\n",
        "            num_first = left_end\n",
        "            num_last = doc_len - right_start\n",
        "            if num_first > 0:\n",
        "                grad_input[i, :left_end, :] = grad_sharded[i, :num_first, :]\n",
        "            if num_last > 0:\n",
        "                grad_input[i, right_start:doc_len, :] = grad_sharded[i, num_first:num_first+num_last, :]\n",
        "        else:  # shard_id == 1\n",
        "            grad_input[i, left_end:right_start, :] = grad_sharded[i, :right_start-left_end, :]\n",
        "    return grad_input\n",
        "\n",
        "# Attention layer using per-document CP sharding\n",
        "class AttnLayerWithPerDocCPSharding(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx,\n",
        "                is_training,\n",
        "                q,\n",
        "                k,\n",
        "                v,\n",
        "                doc_lens,\n",
        "                dropout_p,\n",
        "                softmax_scale,\n",
        "                cp_group,\n",
        "                cp_stream):\n",
        "        \"\"\"\n",
        "        Forward pass for a scaled dot-product attention layer that applies per-document CP sharding.\n",
        "        q, k, v: tensors of shape [B, S, D] where S is the padded sequence length.\n",
        "        doc_lens: 1D tensor of length B with the true document lengths.\n",
        "\n",
        "        The layer first applies per-document CP sharding to q, k, v. Then it computes attention.\n",
        "        \"\"\"\n",
        "        if softmax_scale is None:\n",
        "            softmax_scale = q.size(-1) ** (-0.5)\n",
        "        rank = get_distributed_rank(cp_group)\n",
        "        world_size = 2  # Only two workers supported\n",
        "\n",
        "        # Save padded sequence length from original q, k, v\n",
        "        padded_length = q.size(1)\n",
        "\n",
        "        # Apply per-document CP sharding to q, k, v\n",
        "        sharded_q = per_document_cp_shard_varlen(q, doc_lens, rank, world_size)\n",
        "        sharded_k = per_document_cp_shard_varlen(k, doc_lens, rank, world_size)\n",
        "        sharded_v = per_document_cp_shard_varlen(v, doc_lens, rank, world_size)\n",
        "\n",
        "        # Compute scaled dot-product attention on sharded tokens\n",
        "        attn_scores = torch.matmul(sharded_q, sharded_k.transpose(-2, -1)) * softmax_scale\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        if is_training and dropout_p > 0:\n",
        "            attn_probs = torch.nn.functional.dropout(attn_probs, p=dropout_p, training=True)\n",
        "        output = torch.matmul(attn_probs, sharded_v)\n",
        "\n",
        "        # Save context for backward\n",
        "        ctx.save_for_backward(sharded_q, sharded_k, sharded_v, attn_probs, doc_lens)\n",
        "        ctx.dropout_p = dropout_p\n",
        "        ctx.softmax_scale = softmax_scale\n",
        "        ctx.cp_group = cp_group\n",
        "        ctx.cp_stream = cp_stream\n",
        "        ctx.rank = rank\n",
        "        ctx.world_size = world_size\n",
        "        # Save original padded length\n",
        "        ctx.padded_length = padded_length\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dout):\n",
        "        \"\"\"\n",
        "        Backward pass for the attention layer.\n",
        "        Computes gradients for sharded q, k, v and then maps them back to the original full inputs.\n",
        "        \"\"\"\n",
        "        sharded_q, sharded_k, sharded_v, attn_probs, doc_lens = ctx.saved_tensors\n",
        "        dropout_p = ctx.dropout_p\n",
        "        softmax_scale = ctx.softmax_scale\n",
        "        rank = ctx.rank\n",
        "        world_size = ctx.world_size\n",
        "        padded_length = ctx.padded_length\n",
        "\n",
        "        # Compute gradients for sharded_v\n",
        "        d_sharded_v = torch.matmul(attn_probs.transpose(-2, -1), dout)\n",
        "\n",
        "        # Gradients for attention scores\n",
        "        d_attn = torch.matmul(dout, sharded_v.transpose(-2, -1))\n",
        "        # Backprop through softmax\n",
        "        d_softmax = attn_probs * (d_attn - (d_attn * attn_probs).sum(dim=-1, keepdim=True))\n",
        "        d_softmax = d_softmax * softmax_scale\n",
        "\n",
        "        d_sharded_q = torch.matmul(d_softmax, sharded_k)\n",
        "        d_sharded_k = torch.matmul(d_softmax.transpose(-2, -1), sharded_q)\n",
        "\n",
        "        # Map gradients from the sharded space back to full inputs\n",
        "        dq = per_document_cp_shard_varlen_backward(d_sharded_q, doc_lens, padded_length, rank, world_size)\n",
        "        dk = per_document_cp_shard_varlen_backward(d_sharded_k, doc_lens, padded_length, rank, world_size)\n",
        "        dv = per_document_cp_shard_varlen_backward(d_sharded_v, doc_lens, padded_length, rank, world_size)\n",
        "\n",
        "        # Forward inputs: (is_training, q, k, v, doc_lens, dropout_p, softmax_scale, cp_group, cp_stream)\n",
        "        return None, dq, dk, dv, None, None, None, None, None\n",
        "\n",
        "# Wrapper\n",
        "def attn_layer_with_perdoc_cp_sharding(is_training, q, k, v, doc_lens, dropout_p, softmax_scale, cp_group, cp_stream):\n",
        "    return AttnLayerWithPerDocCPSharding.apply(is_training, q, k, v, doc_lens, dropout_p, softmax_scale, cp_group, cp_stream)\n",
        "\n",
        "# Test usage\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # 4 documents with real text\n",
        "    documents = [\n",
        "        # Document about England (58 tokens)\n",
        "        \"England is a country in the United Kingdom, known for its rich history, iconic landmarks such as Big Ben, the Tower of London, and Buckingham Palace. Its cities, especially London, offer vibrant arts, literature, and culinary scenes, while the countryside features rolling hills, quaint villages, and centuries-old castles.\",\n",
        "        # Document about France (57 tokens)\n",
        "        \"France is renowned for its art, fashion, and culture, with a history that dates back centuries. The country offers stunning architecture like the Eiffel Tower and the Louvre, picturesque countryside, and gourmet cuisine that has influenced the world. From the vineyards of Bordeaux to the beaches of the Riviera, France captivates visitors with its romantic charm.\",\n",
        "        # Document about Germany (58 tokens)\n",
        "        \"Germany is a country known for its engineering, innovation, and rich cultural heritage. It boasts a blend of modern cities like Berlin and Munich with historic sites such as the Brandenburg Gate and ancient castles. Germany's economy is one of the strongest in Europe, and its traditions in music, philosophy, and automotive technology continue to influence the world.\",\n",
        "        # Document about Poland (35 tokens)\n",
        "        \"Poland is a nation with a vibrant history and dynamic cultural scene. From medieval castles and historic cities like Krakow and Gdansk to modern economic growth, Poland offers a fascinating mixture of old and new.\"\n",
        "    ]\n",
        "\n",
        "    # Tokenize documents (simple whitespace split)\n",
        "    token_lists = [doc.split() for doc in documents]\n",
        "    doc_lengths = [len(tokens) for tokens in token_lists]\n",
        "    B = len(documents)\n",
        "    S = max(doc_lengths)  # padded sequence length\n",
        "    D = 8  # embedding dimension\n",
        "\n",
        "    print(\"Document token counts:\", doc_lengths)\n",
        "    print(\"Padded sequence length:\", S)\n",
        "\n",
        "    # Create a padded input tensor.\n",
        "    # For each document, simulate embeddings with random values.\n",
        "    padded_inputs = torch.zeros(B, S, D)\n",
        "    for i, tokens in enumerate(token_lists):\n",
        "        L_i = len(tokens)\n",
        "        padded_inputs[i, :L_i, :] = torch.randn(L_i, D)\n",
        "    padded_inputs.requires_grad_()\n",
        "\n",
        "    # Convert doc_lengths to tensor.\n",
        "    doc_lens_tensor = torch.tensor(doc_lengths)\n",
        "\n",
        "    dropout_p = 0.0  # Set dropout=0 for testing\n",
        "    softmax_scale = None  # Let the layer compute it automatically.\n",
        "    cp_group = {'rank': 0}  # Simulate worker 0.\n",
        "    cp_stream = DummyCPStream()\n",
        "\n",
        "    print(\"\\n--- Simulating Worker 0 with Real Documents ---\")\n",
        "    out_worker0 = attn_layer_with_perdoc_cp_sharding(True, padded_inputs, padded_inputs, padded_inputs,\n",
        "                                                     doc_lens_tensor, dropout_p, softmax_scale, cp_group, cp_stream)\n",
        "    # print(\"Output (worker 0):\", out_worker0)\n",
        "    print(\"Output shape (worker 0):\", out_worker0.shape)\n",
        "    print(\"Expected output shape for worker0 is [4, 29, 8]\")  # [4 documents, max 29 tokens, 8 features]\n",
        "\n",
        "    loss0 = out_worker0.sum()\n",
        "    loss0.backward()\n",
        "    print(\"Gradients for padded_inputs after worker 0 backward:\", padded_inputs.grad.shape)\n",
        "    print(\"Expected gradient shape for padded_inputs should be [4, S, 8] where S =\", S)\n",
        "\n",
        "    # ----- Simulate Worker 1 -----\n",
        "    padded_inputs.grad.zero_()\n",
        "    cp_group['rank'] = 1  # Switch to worker 1.\n",
        "\n",
        "    print(\"\\n--- Simulating Worker 1 with Real Documents ---\")\n",
        "    out_worker1 = attn_layer_with_perdoc_cp_sharding(True, padded_inputs, padded_inputs, padded_inputs,\n",
        "                                                     doc_lens_tensor, dropout_p, softmax_scale, cp_group, cp_stream)\n",
        "    # print(\"Output (worker 1):\", out_worker1)\n",
        "    print(\"Output shape (worker 1):\", out_worker1.shape)\n",
        "    print(\"Expected output shape for worker1 is [4, 29, 8]\")  # [4 documents, max 29 tokens, 8 features]\n",
        "\n",
        "    loss1 = out_worker1.sum()\n",
        "    loss1.backward()\n",
        "    print(\"Gradients for padded_inputs after worker 1 backward:\", padded_inputs.grad.shape)\n",
        "    print(\"Expected gradient shape for padded_inputs should be [4, S, 8] where S =\", S)\n"
      ],
      "metadata": {
        "id": "QU0M99vKTsKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a7d469-7df1-478e-9061-56ba3af97f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document token counts: [48, 56, 58, 35]\n",
            "Padded sequence length: 58\n",
            "\n",
            "--- Simulating Worker 0 with Real Documents ---\n",
            "Output shape (worker 0): torch.Size([4, 29, 8])\n",
            "Expected output shape for worker0 is [4, 29, 8]\n",
            "Gradients for padded_inputs after worker 0 backward: torch.Size([4, 58, 8])\n",
            "Expected gradient shape for padded_inputs should be [4, S, 8] where S = 58\n",
            "\n",
            "--- Simulating Worker 1 with Real Documents ---\n",
            "Output shape (worker 1): torch.Size([4, 29, 8])\n",
            "Expected output shape for worker1 is [4, 29, 8]\n",
            "Gradients for padded_inputs after worker 1 backward: torch.Size([4, 58, 8])\n",
            "Expected gradient shape for padded_inputs should be [4, S, 8] where S = 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now try with a distributed environment where number of workers is not necessarily 2 and with CUDA streams"
      ],
      "metadata": {
        "id": "Ts7t6ee1U6Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Function\n",
        "import math\n",
        "\n",
        "# Dummies for simulating distributed environment\n",
        "def get_distributed_rank(cp_group):\n",
        "    # For testing, assume cp_group is a dict with key 'rank'\n",
        "    return cp_group['rank']\n",
        "\n",
        "class DummyCPStream:\n",
        "    def wait_stream(self, stream):\n",
        "        stream.synchronize()  # simple synchronization\n",
        "\n",
        "# Per-document CP sharding (variable-length) implementation for general world_size\n",
        "class PerDocumentCPShardingVarLenGeneral(Function):\n",
        "    # store the indices_info in a class variable so that it can be retrieved later\n",
        "    last_indices_info = None\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, doc_lens, shard_id, world_size):\n",
        "        \"\"\"\n",
        "        For each document in the batch:\n",
        "          - Let N = 2 * world_size.\n",
        "          - Compute seg_len_base = doc_len // N and rem = doc_len % N\n",
        "          - For worker with rank r:\n",
        "              * Segment 0 indices:\n",
        "                    start0 = r * seg_len_base + min(r, rem)\n",
        "                    end0   = start0 + seg_len_base + (1 if r < rem else 0)\n",
        "              * Segment 1 indices:\n",
        "                    Let r2 = N - r - 1.\n",
        "                    start1 = r2 * seg_len_base + min(r2, rem)\n",
        "                    end1   = start1 + seg_len_base + (1 if r2 < rem else 0)\n",
        "          - For each document, return two tensors: seg0 and seg1, each padded to the maximum length across the batch\n",
        "        \"\"\"\n",
        "        B, padded_length, D = input.shape\n",
        "        seg0_list = []\n",
        "        seg1_list = []\n",
        "        seg0_lengths = []\n",
        "        seg1_lengths = []\n",
        "        indices_info = []\n",
        "        N = 2 * world_size\n",
        "        for i in range(B):\n",
        "            L_i = int(doc_lens[i])\n",
        "            seg_len_base = L_i // N\n",
        "            rem = L_i % N\n",
        "            r = shard_id\n",
        "            extra0 = 1 if r < rem else 0\n",
        "            start0 = r * seg_len_base + min(r, rem)\n",
        "            end0 = start0 + seg_len_base + extra0\n",
        "            r2 = N - r - 1\n",
        "            extra1 = 1 if r2 < rem else 0\n",
        "            start1 = r2 * seg_len_base + min(r2, rem)\n",
        "            end1 = start1 + seg_len_base + extra1\n",
        "            seg0 = input[i, :L_i, :][start0:end0, :]\n",
        "            seg1 = input[i, :L_i, :][start1:end1, :]\n",
        "            seg0_list.append(seg0)\n",
        "            seg1_list.append(seg1)\n",
        "            seg0_lengths.append(seg0.size(0))\n",
        "            seg1_lengths.append(seg1.size(0))\n",
        "            indices_info.append({'start0': start0, 'end0': end0,\n",
        "                                 'start1': start1, 'end1': end1,\n",
        "                                 'doc_len': L_i})\n",
        "        max_seg0 = max(seg0_lengths)\n",
        "        max_seg1 = max(seg1_lengths)\n",
        "        seg0_padded = input.new_zeros((B, max_seg0, D))\n",
        "        seg1_padded = input.new_zeros((B, max_seg1, D))\n",
        "        for i, seg0 in enumerate(seg0_list):\n",
        "            seg0_padded[i, :seg0.size(0), :] = seg0\n",
        "        for i, seg1 in enumerate(seg1_list):\n",
        "            seg1_padded[i, :seg1.size(0), :] = seg1\n",
        "        # Save indices_info in both ctx and as a class variable for later retrieval\n",
        "        ctx.indices_info = indices_info\n",
        "        PerDocumentCPShardingVarLenGeneral.last_indices_info = indices_info\n",
        "        ctx.input_shape = input.shape  # [B, padded_length, D]\n",
        "        return seg0_padded, seg1_padded\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_seg0, grad_seg1):\n",
        "        \"\"\"\n",
        "        For each document, scatter gradients from grad_seg0 and grad_seg1 back into\n",
        "        a gradient tensor of shape [B, padded_length, D] using the saved indices\n",
        "        \"\"\"\n",
        "        B, padded_length, D = ctx.input_shape\n",
        "        grad_input = grad_seg0.new_zeros(B, padded_length, D)\n",
        "        indices_info = ctx.indices_info\n",
        "        for i, info in enumerate(indices_info):\n",
        "            start0 = info['start0']\n",
        "            end0 = info['end0']\n",
        "            start1 = info['start1']\n",
        "            end1 = info['end1']\n",
        "            seg0_len = end0 - start0\n",
        "            seg1_len = end1 - start1\n",
        "            grad_input[i, start0:end0, :] = grad_seg0[i, :seg0_len, :]\n",
        "            grad_input[i, start1:end1, :] = grad_seg1[i, :seg1_len, :]\n",
        "        return grad_input, None, None, None\n",
        "\n",
        "def per_document_cp_shard_varlen_general(input, doc_lens, shard_id, world_size):\n",
        "    return PerDocumentCPShardingVarLenGeneral.apply(input, doc_lens, shard_id, world_size)\n",
        "\n",
        "# Backward helper for mapping gradients from segments back to full input\n",
        "def per_document_cp_shard_varlen_general_backward(grad_seg0, grad_seg1, indices_info, padded_length):\n",
        "    \"\"\"\n",
        "    For each document, using saved indices from forward, scatter gradients from\n",
        "    grad_seg0 and grad_seg1 into a gradient tensor of shape [B, padded_length, D]\n",
        "    \"\"\"\n",
        "    B, _, D = grad_seg0.size()\n",
        "    grad_input = grad_seg0.new_zeros(B, padded_length, D)\n",
        "    for i, info in enumerate(indices_info):\n",
        "        start0 = info['start0']\n",
        "        end0 = info['end0']\n",
        "        start1 = info['start1']\n",
        "        end1 = info['end1']\n",
        "        seg0_len = end0 - start0\n",
        "        seg1_len = end1 - start1\n",
        "        grad_input[i, start0:end0, :] = grad_seg0[i, :seg0_len, :]\n",
        "        grad_input[i, start1:end1, :] = grad_seg1[i, :seg1_len, :]\n",
        "    return grad_input\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Attention layer using per-document CP sharding with CUDA streams\n",
        "# ------------------------------------------------------------------\n",
        "class AttnLayerWithPerDocCPShardingGeneral(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx,\n",
        "                is_training,\n",
        "                q,\n",
        "                k,\n",
        "                v,\n",
        "                doc_lens,\n",
        "                dropout_p,\n",
        "                softmax_scale,\n",
        "                cp_group,\n",
        "                cp_stream,\n",
        "                world_size):\n",
        "        \"\"\"\n",
        "        Scaled dot-product attention layer that applies per-document CP sharding with CUDA streams\n",
        "        q, k, v: tensors of shape [B, S, D] (S = padded sequence length)\n",
        "        doc_lens: 1D tensor of length B with true document lengths.\n",
        "        world_size: number of workers in the distributed environment.\n",
        "        \"\"\"\n",
        "        if softmax_scale is None:\n",
        "            softmax_scale = q.size(-1) ** (-0.5)\n",
        "        rank = get_distributed_rank(cp_group)\n",
        "        B, padded_length, D = q.shape\n",
        "\n",
        "        # Apply per-document CP sharding to q, k, v using the general function\n",
        "        q_seg0, q_seg1 = per_document_cp_shard_varlen_general(q, doc_lens, rank, world_size)\n",
        "        k_seg0, k_seg1 = per_document_cp_shard_varlen_general(k, doc_lens, rank, world_size)\n",
        "        v_seg0, v_seg1 = per_document_cp_shard_varlen_general(v, doc_lens, rank, world_size)\n",
        "\n",
        "        # Create two CUDA streams\n",
        "        stream0 = torch.cuda.Stream()\n",
        "        stream1 = torch.cuda.Stream()\n",
        "\n",
        "        # Process segment 0 and segment 1 concurrently\n",
        "        with torch.cuda.stream(stream0):\n",
        "            attn_scores0 = torch.matmul(q_seg0, k_seg0.transpose(-2, -1)) * softmax_scale\n",
        "            attn_probs0 = torch.softmax(attn_scores0, dim=-1)\n",
        "            if is_training and dropout_p > 0:\n",
        "                attn_probs0 = torch.nn.functional.dropout(attn_probs0, p=dropout_p, training=True)\n",
        "            out_seg0 = torch.matmul(attn_probs0, v_seg0)\n",
        "\n",
        "        with torch.cuda.stream(stream1):\n",
        "            attn_scores1 = torch.matmul(q_seg1, k_seg1.transpose(-2, -1)) * softmax_scale\n",
        "            attn_probs1 = torch.softmax(attn_scores1, dim=-1)\n",
        "            if is_training and dropout_p > 0:\n",
        "                attn_probs1 = torch.nn.functional.dropout(attn_probs1, p=dropout_p, training=True)\n",
        "            out_seg1 = torch.matmul(attn_probs1, v_seg1)\n",
        "\n",
        "        # Synchronize streams\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Concatenate the two segments along the sequence dimension\n",
        "        output = torch.cat([out_seg0, out_seg1], dim=1)  # shape: [B, (max_seg0 + max_seg1), D]\n",
        "\n",
        "        # Save context for backward.\n",
        "        # Save all necessary tensors for computing gradients through attention.\n",
        "        ctx.save_for_backward(q_seg0, q_seg1, k_seg0, k_seg1, v_seg0, v_seg1, doc_lens, attn_probs0, attn_probs1)\n",
        "        # Retrieve indices_info stored by the sharding function.\n",
        "        ctx.indices_info = PerDocumentCPShardingVarLenGeneral.last_indices_info\n",
        "        ctx.dropout_p = dropout_p\n",
        "        ctx.softmax_scale = softmax_scale\n",
        "        ctx.cp_group = cp_group\n",
        "        ctx.cp_stream = cp_stream\n",
        "        ctx.rank = rank\n",
        "        ctx.world_size = world_size\n",
        "        ctx.input_shape = q.shape  # original [B, S, D]\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dout):\n",
        "        \"\"\"\n",
        "        Splits the gradient of the concatenated output into two parts corresponding to\n",
        "        the two segments and computes gradients for q, k, and v.\n",
        "        \"\"\"\n",
        "        # Retrieve saved tensors.\n",
        "        q_seg0, q_seg1, k_seg0, k_seg1, v_seg0, v_seg1, doc_lens, attn_probs0, attn_probs1 = ctx.saved_tensors\n",
        "        dropout_p = ctx.dropout_p\n",
        "        softmax_scale = ctx.softmax_scale\n",
        "        rank = ctx.rank\n",
        "        world_size = ctx.world_size\n",
        "        padded_length = ctx.input_shape[1]\n",
        "\n",
        "        # Determine lengths of segments (from the sharded tensors).\n",
        "        L0 = q_seg0.size(1)\n",
        "        L1 = q_seg1.size(1)\n",
        "        # Split dout into segments corresponding to seg0 and seg1.\n",
        "        dout_seg0 = dout[:, :L0, :]  # shape: [B, L0, D]\n",
        "        dout_seg1 = dout[:, L0:L0+L1, :]  # shape: [B, L1, D]\n",
        "\n",
        "        # --- Compute gradients for segment 0 ---\n",
        "        # dV0 = A0^T @ dout_seg0\n",
        "        d_v_seg0 = torch.matmul(attn_probs0.transpose(-2, -1), dout_seg0)\n",
        "        # dA0 = dout_seg0 @ V0^T\n",
        "        dA0 = torch.matmul(dout_seg0, v_seg0.transpose(-2, -1))\n",
        "        # dZ0 = A0 * (dA0 - sum(dA0 * A0, dim=-1, keepdim=True))\n",
        "        dZ0 = attn_probs0 * (dA0 - (dA0 * attn_probs0).sum(dim=-1, keepdim=True))\n",
        "        dZ0 = dZ0 * softmax_scale\n",
        "        # dQ0 = dZ0 @ K0\n",
        "        d_q_seg0 = torch.matmul(dZ0, k_seg0)\n",
        "        # dK0 = dZ0^T @ Q0\n",
        "        d_k_seg0 = torch.matmul(dZ0.transpose(-2, -1), q_seg0)\n",
        "\n",
        "        # --- Compute gradients for segment 1 ---\n",
        "        d_v_seg1 = torch.matmul(attn_probs1.transpose(-2, -1), dout_seg1)\n",
        "        dA1 = torch.matmul(dout_seg1, v_seg1.transpose(-2, -1))\n",
        "        dZ1 = attn_probs1 * (dA1 - (dA1 * attn_probs1).sum(dim=-1, keepdim=True))\n",
        "        dZ1 = dZ1 * softmax_scale\n",
        "        d_q_seg1 = torch.matmul(dZ1, k_seg1)\n",
        "        d_k_seg1 = torch.matmul(dZ1.transpose(-2, -1), q_seg1)\n",
        "\n",
        "        # Map gradients from segments back to full inputs using the backward helper.\n",
        "        dq = per_document_cp_shard_varlen_general_backward(d_q_seg0, d_q_seg1, ctx.indices_info, padded_length)\n",
        "        dk = per_document_cp_shard_varlen_general_backward(d_k_seg0, d_k_seg1, ctx.indices_info, padded_length)\n",
        "        dv = per_document_cp_shard_varlen_general_backward(d_v_seg0, d_v_seg1, ctx.indices_info, padded_length)\n",
        "        return None, dq, dk, dv, None, None, None, None, None, None\n",
        "\n",
        "def attn_layer_with_perdoc_cp_sharding_general(is_training, q, k, v, doc_lens, dropout_p, softmax_scale, cp_group, cp_stream, world_size):\n",
        "    return AttnLayerWithPerDocCPShardingGeneral.apply(is_training, q, k, v, doc_lens, dropout_p, softmax_scale, cp_group, cp_stream, world_size)\n",
        "\n",
        "# Test usage for the general implementation with fake documents\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # Create dummy inputs: q, k, v of shape [B, S, D]\n",
        "    B = 4\n",
        "    S = 160  # padded sequence length (assume documents up to 160 tokens)\n",
        "    D = 8\n",
        "    q = torch.randn(B, S, D, requires_grad=True)\n",
        "    k = torch.randn(B, S, D, requires_grad=True)\n",
        "    v = torch.randn(B, S, D, requires_grad=True)\n",
        "    # Fake document lengths\n",
        "    doc_lens = torch.tensor([158, 127, 160, 155])\n",
        "\n",
        "    dropout_p = 0.1  # Doesn't matter for gradient computation here\n",
        "    softmax_scale = None\n",
        "    world_size = 4  # Number of workers\n",
        "    # Simulate for worker 1\n",
        "    cp_group = {'rank': 1}\n",
        "    cp_stream = DummyCPStream()\n",
        "\n",
        "    print(\"Document token counts:\", doc_lens.tolist())\n",
        "    print(\"Padded sequence length:\", S)\n",
        "\n",
        "    out_general = attn_layer_with_perdoc_cp_sharding_general(True, q, k, v, doc_lens, dropout_p, softmax_scale, cp_group, cp_stream, world_size)\n",
        "    print(\"Output shape (general):\", out_general.shape)\n",
        "    # Expected output shape: [B, L_worker, D] where L_worker = (max_seg0 + max_seg1) for the batch\n",
        "    print(\"Expected output shape: [4, 40, 8]\")\n",
        "\n",
        "    loss_general = out_general.sum()\n",
        "    loss_general.backward()\n",
        "    print(\"Gradients for q (general) shape:\", q.grad.shape)\n",
        "    print(\"Expected gradients for q shape: [4, 160, 8]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-JdfcLEVDsT",
        "outputId": "c75dbc81-9f3b-44b8-d19e-de8eb2ec4186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document token counts: [158, 127, 160, 155]\n",
            "Padded sequence length: 160\n",
            "Output shape (general): torch.Size([4, 40, 8])\n",
            "Expected output shape: [4, 40, 8]\n",
            "Gradients for q (general) shape: torch.Size([4, 160, 8])\n",
            "Expected gradients for q shape: [4, 160, 8]\n"
          ]
        }
      ]
    }
  ]
}